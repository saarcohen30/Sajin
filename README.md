# 3rd International AIWolf Contest: The Sajin Agent

**Sajin** is an agent participating in the 3rd International [AIWolf](http://aiwolf.org/en/3rd-international-aiwolf-contest) Contest, whose goal is to develop an AI agent for the [Werewolf](https://werewolf.chat/) game, that can perform reliably well against other AI agents.

Out agent is named after [Sajin Komamura (狛村 左陣)](https://bleach.fandom.com/wiki/Sajin_Komamura), a werewolf character from the Japanese anime and manga, referred to as "Bleach".

This repository contains the source code for the **Sajin** agent.

# How does Sajin work ?
The **Sajin** agent follows several baseline behaviours, which are intertwined together so as to yield the creation of the **Sajin** agent.
We provide a brief outline of these concepts in the following subsections.

## Stochastic Bayesian Deep Q-Network (SBDQN)
The proliferation of Reinforcement Learning (RL) algorithms, as means for solving Markov decision process (MDP) problems, has flourished a trade-off between several substantial factors affecting their performance, such as exploration, exploitation, statistical efficiency, computational expense, uncertainty and estimation bias. As a mitigation, we put forth the Stochastic Bayesian Double Deep Q-Network (SBDDQ), a novel model which attempts to bridge the gaps between those challenges. Where each existing method solely solves a subset of such factors, SBDDQ intertwines several techniques so as to make up for the union of their shortcomings. 

## The Reinforcement Learning Framework for the Werewolf Game
The consideration of RL yields the presence of an action and a state set. 

### State Space
Whereas the state set is derived from the [AIWolf Protocols](http://aiwolf.org/control-panel/wp-content/uploads/2021/05/Regulation_2021_1.2.3.pdf), the later is broadly illustrated in this section. The agents' perception of the environment stems from the state space, which is generated by multiple elements. A state is represented by a matrix, extracted from conversation history. Such a matrix could be roughly divided into two central components, on which we elaborate in the following subsections.

#### Attitude
We regard the attitude of player `i` towards player `j`, as outlined subsequently:
- `SpecialAbility(i,j)` -- Whether player `i` said he utilized his special ability for player `j` or not. Clearly, this feature applies to either a Seer, a Medium or a Bodyguard. which own a special ability.
- `Vote(i,j)` -- Whether player `i` said he voted for player `j` or not.
- `Estimate(i,j)` -- Whether player `i` estimated that player `j` is a werewolf or not. 
- `Guarded(i,j)` -- Whether player `i`, which is a bodyguard, managed to guard player `j`.
Features of this sort are all assigned with a Boolean value of either `0` or `1`, and are stored in a three-dimensional array, denoted by `x_3d`. Each specific attitude is assigned with a numerical value `l` (as depicted in the implementation), where `x_3d`'s `(i,j,l)`-th element corresponds to `i`'s `l`-th attitude towards `j`.

#### Declarative Features
Refers to player `i`'s declared status, which is comprised of the following: 
- `Role(i)` -- The confirmed fact about player `i`'s true role, which constitutes a consequence of its coming out action, if it was incurred.
- `Attacked(i)` -- Whether player `i` was attacked.
- `Executed(i,j)` -- Whether player `i` was executed. 
Such features are all assigned with a Boolean value of either 0 or 1, and are stored in a two-dimensional array, denoted by `x_2d`. Each specific declaration is assigned with a numerical value `l` (as depicted in the implementation), where `x_2d`'s `(i,l)`-th element corresponds to `i`'s `l`-th declarative feature.

### Design of Rewards
Rewards vastly contribute to the development of new strategies, as each agent aims at executing actions which maximize its expected reward. Specifically, each agent is either rewarded or penalized under the following conditions:
#### During each day
The number of days comprising an entire match plays a critical role in the game. So as to encourages agents to win faster, at the end of each day every agent is penalized by a small factor (-1).
#### Death
Death of an agent supplies it with a penalty of -5. Such a penalty avoids a high percentage of suicides, which clearly constitute as strategies in some matches.
#### Preferences of targets
During each "Vote" phase, every agent exhibits different preferences for choosing whom to vote for. The coordinated behaviour profound by the Werewolf game is largely attributed to such preferences. For encouraging a cooperative behaviour, agents are penalized when voting for an agent that is not killed. 
#### Winning or Losing
At the end of a match, the winning team is rewarded by a factor of 25, whereas the losing team is penalized by a factor of -25.
#### Divination results
The proliferation of agents' strategies is vastly affected by the utilization of special abilities. Seers should be rewarded for each divination action, yet more encouraged to divine werewolves. Seers are thus rewarded by a factor of 15, 10, or 2 for divining an agent who came out as a Werewolf, Possessed or Villager (respectively). Yet, the special ability incurred by Bodyguards does not involve any feedback regarding the true role of the guarded agent, and thus such action cannot be explicitly rewarded. The same is satisfied for Mediums, whose special ability is executed upon the agent eliminated by voting.

### DQN for a Game with 15 Players
The three-dimensional array `x_3d` is first fed into a 32 `Conv1D` filters with appropriate input shapes, whose result is then processed by a `ReLU` activation function, then fed into 9 additional `Conv1D` filters and a ReLU activation function is applied on the attained result. The final output is then reshaped into a `15 X 15 X 3 X 3` tensor, and a `tf.matmul` operation is executed with respect to a matrix `t3d_mat`, enumerating all 5460 (15!) possible permutations of agents' roles. Note that a similar process is administered upon the two-dimensional array `x_2d`, where an addition is performed between the two final outputs. The addition's result is fed through a softmax function, whose output is denoted by `p`. Given `p`, the DQN's final outcome is given by:
`tf.tensordot(t2d, p / K.sum(p), axes = [0, 0]).transpose()`.
Finally, we assign a Bayesian Linear Regression (BLR) layer on the top of the representation.

### DQN for a Game with 5 Players
The DQN for a setup of 5 players is somewhat similar, yet it consists of minor exception. The input arrays, `x_3d` and `x_2d`, are first concatenated to form a single `(1,1,340)` tensor, which is then fed into 512 `Conv1D` filters with appropriate input shapes, whose result is then processed by a `ReLU` activation function, then fed into 256 additional `Conv1D` filters and a ReLU activation function is applied on the attained result. The final output is fed into 60 `Conv1D` filters, and then processed by a Softmax function, whose output is reshaped to a vector that is 60 in length.  Denoting this vector by `p`, the DQN's final outcome is given as in the 15 players setup. Equivalently to the 15 players setup, we assign a BLR layer on the top of the representation. 

## Metrics
So as to elevate the characteristics flourished by the SBDQN algorithm, we herein refer to a substantial issue, affecting the strategies' quality. When the pre-trained policy is executed in practice, it can be regarded as a hard-coded behaviour, which yields a fixed baseline along the evaluation. So as to insert an adaptive nature into the previously trained policy, we suggest to incorporate several metrics, so as to strengthen the overall policy. Readers should refer to Subsection \ref{sec:Sajin.py} for further elaboration - **Winning Counter Metric.** Accurately, the strength of players' behavior can be measured via the number of matches for each kind of role, in which players of this specific role won. This knowledge can be considered along the game in an agent's decision on whom to execute, and, in the case of werewolf-aligned player, whom to attack.
- **Coming Out Metric.** Along the game's progression, agents may choose to willingly come out, according to either their true role or another one. However, there are three roles, where each can be solely associated with a single agent: Seer, Medium and Bodyguard. Thus, if two or more agents were to come out as one of this roles, one of them is necessarily lying. Consequently, villager-aligned players could utilize such information, so as to vote for the agent which is less likely to be of the certain role. For instance, if two agents were to come out as a Seer, a villager-aligned agent should vote for the player which is most likely to be a Werewolf. 
- **Divination Metric.** Exploiting its special ability, a Seer might prefer to vote for agents divined as either a werewolf or a possessed agent. Alternately, if villager-aligned agents were divined, and there isn't any alive werewolf-aligned agent for which a divination result was previously obtained, a Seer should vote for any agent which does not reside in the divination set of villager-aligned ones.

# Contents of this repo

- [/server](server) : the AIWolf server that run Werewolf games with several agents
- [/killerQueen](killerQueen) : code source of the Killer Queen agent
- [/other_agents](other_agents) : code source of other public agents, for testing purpose
- [/archive](archive) : older versions of Killer Queen

# How to run a test game

1. Configure the game's settings in `gameSettings.ini`. follow the format specified to choose games composition (agents algorithms and roles). Here are useful parameters:
  - `game` (int) : number of games to simulate

  - `log` (path) : path to the folder where the server will write game logs

  - `view` (bool) : use the GUI to follow the course of the game (Japanese)


2. Start the server.
* Windows : launch [RUN.bat](RUN.bat) or [server/AutoStarter.bat](server/AutoStarter.bat)
* Linux : launch [RUN.sh](RUN.sh) or [server/AutoStarter.bat](server/AutoStarter.bat)
