# 3rd International AIWolf Contest: The Sajin Agent

**Sajin** is an agent participating in the 3rd International [AIWolf](http://aiwolf.org/en/3rd-international-aiwolf-contest) Contest, whose goal is to develop an AI agent for the [Werewolf](https://werewolf.chat/) game, that can perform reliably well against other AI agents.

Out agent is named after [Sajin Komamura (狛村 左陣)](https://bleach.fandom.com/wiki/Sajin_Komamura), a werewolf character from the Japanese anime and manga, referred to as "Bleach".

This repository contains the source code for the **Sajin** agent.

# How does Sajin work ?
The **Sajin** agent follows several baseline behaviours, which are intertwined together so as to yield the creation of the **Sajin** agent.
We provide a brief outline of these concepts in the following subsections.

## Stochastic Bayesian Deep Q-Network (SBDQN)
The proliferation of Reinforcement Learning (RL) algorithms, as means for solving Markov decision process (MDP) problems, has flourished a trade-off between several substantial factors affecting their performance, such as exploration, exploitation, statistical efficiency, computational expense, uncertainty and estimation bias. As a mitigation, we put forth the Stochastic Bayesian Double Deep Q-Network (SBDDQ), a novel model which attempts to bridge the gaps between those challenges. Where each existing method solely solves a subset of such factors, SBDDQ intertwines several techniques so as to make up for the union of their shortcomings. 

## The Reinforcement Learning Framework for the Werewolf Game
The consideration of RL yields the presence of an action and a state set. 

### State Space
Whereas the state set is derived from the [AIWolf Protocols](http://aiwolf.org/control-panel/wp-content/uploads/2021/05/Regulation_2021_1.2.3.pdf), the later is broadly illustrated in this section. The agents' perception of the environment stems from the state space, which is generated by multiple elements. A state is represented by a matrix, extracted from conversation history. Such a matrix could be roughly divided into two central components, on which we elaborate in the following subsections.

#### Attitude
We regard the attitude of player `i` towards player `j`, as outlined subsequently:
- `SpecialAbility(i,j)` -- Whether player `i` said he utilized his special ability for player `j` or not. Clearly, this feature applies to either a Seer, a Medium or a Bodyguard. which own a special ability.
- `Vote(i,j)` -- Whether player `i` said he voted for player `j` or not.
- `Estimate(i,j)` -- Whether player `i` estimated that player `j` is a werewolf or not. 
- `Guarded(i,j)` -- Whether player `i`, which is a bodyguard, managed to guard player `j`.
Features of this sort are all assigned with a Boolean value of either `0` or `1`, and are stored in a three-dimensional array, denoted by `x_3d`. Each specific attitude is assigned with a numerical value `l` (as depicted in the implementation), where `x_3d`'s `(i,j,l)`-th element corresponds to `i`'s `l`-th attitude towards `j`.

#### Declarative Features
Refers to player `i`'s declared status, which is comprised of the following: 
- `Role(i)` -- The confirmed fact about player `i`'s true role, which constitutes a consequence of its coming out action, if it was incurred.
- `Attacked(i)` -- Whether player `i` was attacked.
- `Executed(i,j)` -- Whether player `i` was executed. 
Such features are all assigned with a Boolean value of either 0 or 1, and are stored in a two-dimensional array, denoted by `x_2d`. Each specific declaration is assigned with a numerical value `l` (as depicted in the implementation), where `x_2d`'s `(i,l)`-th element corresponds to `i`'s `l`-th declarative feature.

### Design of Rewards
Rewards vastly contribute to the development of new strategies, as each agent aims at executing actions which maximize its expected reward. Specifically, each agent is either rewarded or penalized under the following conditions:
#### During each day
The number of days comprising an entire match plays a critical role in the game. So as to encourages agents to win faster, at the end of each day every agent is penalized by a small factor (-1).
#### Death
Death of an agent supplies it with a penalty of -5. Such a penalty avoids a high percentage of suicides, which clearly constitute as strategies in some matches.
#### Preferences of targets
During each "Vote" phase, every agent exhibits different preferences for choosing whom to vote for. The coordinated behaviour profound by the Werewolf game is largely attributed to such preferences. For encouraging a cooperative behaviour, agents are penalized when voting for an agent that is not killed. 
#### Winning or Losing
At the end of a match, the winning team is rewarded by a factor of 25, whereas the losing team is penalized by a factor of -25.
#### Divination results
The proliferation of agents' strategies is vastly affected by the utilization of special abilities. Seers should be rewarded for each divination action, yet more encouraged to divine werewolves. Seers are thus rewarded by a factor of 15, 10, or 2 for divining an agent who came out as a Werewolf, Possessed or Villager (respectively). Yet, the special ability incurred by Bodyguards does not involve any feedback regarding the true role of the guarded agent, and thus such action cannot be explicitly rewarded. The same is satisfied for Mediums, whose special ability is executed upon the agent eliminated by voting.

### DQN for a Game with 15 Players
The three-dimensional array `x_3d` is first fed into a 32 `Conv1D` filters with appropriate input shapes, whose result is then processed by a `ReLU` activation function, then fed into 9 additional `Conv1D` filters and a ReLU activation function is applied on the attained result. The final output is then reshaped into a `15 X 15 X 3 X 3` tensor, and a `tf.matmul` operation is executed with respect to a matrix `t3d_mat`, enumerating all 5460 (15!) possible permutations of agents' roles. Note that a similar process is administered upon the two-dimensional array `x_2d`, where an addition is performed between the two final outputs. The addition's result is fed through a softmax function, whose output is denoted by `p`. Given `p`, the DQN's final outcome is given by:
`tf.tensordot(t2d, p / K.sum(p), axes = [0, 0]).transpose()`.
Finally, we assign a Bayesian Linear Regression (BLR) layer on the top of the representation.

### DQN for a Game with 5 Players
The DQN for a setup of 5 players is somewhat similar, yet it consists of minor exception. The input arrays, `x_3d` and `x_2d`, are first concatenated to form a single `(1,1,340)` tensor, which is then fed into 512 `Conv1D` filters with appropriate input shapes, whose result is then processed by a `ReLU` activation function, then fed into 256 additional `Conv1D` filters and a ReLU activation function is applied on the attained result. The final output is fed into 60 `Conv1D` filters, and then processed by a Softmax function, whose output is reshaped to a vector that is 60 in length.  Denoting this vector by `p`, the DQN's final outcome is given as in the 15 players setup. Equivalently to the 15 players setup, we assign a BLR layer on the top of the representation. 

## Metrics
So as to elevate the characteristics flourished by the SBDQN algorithm, we herein refer to a substantial issue, affecting the strategies' quality. When the pre-trained policy is executed in practice, it can be regarded as a hard-coded behaviour, which yields a fixed baseline along the evaluation. So as to insert an adaptive nature into the previously trained policy, we suggest to incorporate several metrics, so as to strengthen the overall policy.
- **Winning Counter Metric.** Accurately, the strength of players' behavior can be measured via the number of matches for each kind of role, in which players of this specific role won. This knowledge can be considered along the game in an agent's decision on whom to execute, and, in the case of werewolf-aligned player, whom to attack.
- **Coming Out Metric.** Along the game's progression, agents may choose to willingly come out, according to either their true role or another one. However, there are three roles, where each can be solely associated with a single agent: Seer, Medium and Bodyguard. Thus, if two or more agents were to come out as one of this roles, one of them is necessarily lying. Consequently, villager-aligned players could utilize such information, so as to vote for the agent which is less likely to be of the certain role. For instance, if two agents were to come out as a Seer, a villager-aligned agent should vote for the player which is most likely to be a Werewolf. 
- **Divination Metric.** Exploiting its special ability, a Seer might prefer to vote for agents divined as either a werewolf or a possessed agent. Alternately, if villager-aligned agents were divined, and there isn't any alive werewolf-aligned agent for which a divination result was previously obtained, a Seer should vote for any agent which does not reside in the divination set of villager-aligned ones.

## Self Deep Parameter Transfer DRL (SDPT-DRL)
The SDPT-DRL technique is outlined as follows. First, the DL-based agent is trained in the Werewolf game environment. Then, the parameters learned in this non-RL setting are transferred to the RLobo agent as an initialization of its own parameters. We note that, unlike most of the existing literature on transfer learning, the source and target tasks are the same. However, a recent work by [Plisnier et al.](https://ala2021.vub.ac.be/papers/ALA2021_paper_24.pdf) demonstrated that an RL agent performing this “self-transfer” significantly improves its total sample-efficiency and the final policy quality, compared to what it would have achieved by learning the task from scratch. Yet, we emphasize that their self-transfer method concerns a pair of RL-based agents, while ours does not, as mentioned earlier.

## Mixture Policy
While the various multi-objective algorithms learn only a finite set of deterministic non-stationary policies, one could employ a  stochastic combination of these policies. Particularly, such a stochastic combination of policies is refereed to as a [**mixture policy**](https://www.academia.edu/download/49991751/Constructing_Stochastic_Mixture_Policies20161030-2646-1jjv5if.pdf). Thus, although there are only two deterministic policies for the original problem, a mixture policy implicates that we can sample the entire convex hull of policies by combining the deterministic policies with a certain probability. Hence, stochastic combinations of the policies of the Pareto front can represent every solution on the convex hull of the same Pareto front.

# Implementation Details
We drew inspiration from the [cash](https://github.com/k-harada/AIWolfPy) team, whose initial participation in the competition occurred at [GAT2017](http://aiwolf.org/archives/1367). Further, our own agent is based on their agent during the subsequent competition (CEDEC2017, where their implementation can be found at: \url{http://aiwolf.org/control-panel/wp-content/uploads/2017/09/aiwolf2017_cash.zip}). Yet, their agent constitutes a basic foundation for our Sajin agent, and we have majorly altered their code. Thus, we herein provide a concise description of such alterations.

## [`aiwolfpy/read_log.py`]() 
The `read_log.py` was originally provided as part of the [AIWolfPy Library](https://github.com/aiwolf/AIWolfPy), supplied by the contest's managers and consists of the `read_log(log_path, n_agents)` method, which receives the path to the game log and the number of agents. Yet, this file neglected processing several parts of a specific game log, which are crucial for performing our proposed framework. The alterations needed are as follows:
- During the parsing of the lines corresponding to the first day (Day 0), we identify the ID of the agent, which was proven to be the strongest during ANAC2020.
- At each `'status'` line, depicting whether players are alive or dead, we record the line in the data frame to be returned during the end of the `read_log` method. If such a line depicts the death of certain player, we further generate a mapping of its ID to its role. 
- At `'result'` line, depicting which team won the game, we check which team won and record the number of days across which the game was executed.

Finally, the output of the `read_log` method comprises of a data frame illustrating the players' actions, a role map of dead agents, the identity of the winning team, how many days the game lasted, and the ID of the strongest agent in the game log.

## [`aiwolfpy/cash/dqn5.py`]() & [`aiwolfpy/cash/dqn15.py`]() 
The `predictor_5.py` and `predictor_15.py` files implement the `Predictor_5` and `Predictor_15` classes, which constitute the predictors for a setup of 5 and 15 players, respectively. Their respective code is highly based on the one supplied by the **cash** team, yet their code was not suited for the utilization of the `DQNetwork5` and `DQNetwork15` classes. We thus altered the `initialize` method by constructing an online DQN and a target DQN, where the target DQN's parameters are set to be those of the online DQN. For this sake, we added the `update_target_dqn` method.

## [`blr.py`]() 
Implements a BLR layer. The most crucial part of this file is the `sample_W` method, which performs the Thompson sampling from the posterior distribution. The other methods follow the usual BLR baseline and perform the updating of its parameters.

## [`utils.py`]() 
Contains several auxiliary methods and classes, utilized across the entire code, which are as follows:

### Replay Buffer
We provide a class implementing a standard replay buffer.
### Rewards
We create a dictionary `rewards`, comprising of the rewards and penalties administered under the circumstances depicted in Subsection \ref{sec:rewards}.

### Action Methods
We have implemented several methods, which perform the agent selection process required as a part of the respective action. For instance, the `vote_possessed_15` method implements the voting action of a Possessed player in a setup of 15 players. Clearly, those methods are of critical importance during both the execution phase of our agents.

### Hyperparameters
We implement the `Options` class, consisting of initialization for all hyperparamers, incorporated in our model.

## [`PretrainedSajin.py`]() 
This Python file provides the implementation of the pre-training phase of the incorporated policy. In can be roughly divided into two parts: training a policy in a game of 15 players, and then in a game of 5 players. Each trained policy follows a similar process: parsing the game log (via `read_log.py`) so as to attain the data required for learning, iterating over the game's days, while executing the actions performed according to the log and learning from experiences incurred with respect to those actions. The following are central auxiliary methods, which play a pivotal role in this learning process:

### `game_initializer(df, agent=1)`
This method receives the data frame corresponding to the game, as well as a player's ID. It first determines the player's role in the regarded game, and then filters the data frame `df` in a manner that it solely consists of legitimate actions for the player's role. 

### `game_data_filter(df, day, predictor, phase='daily_initialize', agent=1)`
This method receives the data frame corresponding to the game, the number of day, a predictor, the phase in the game and a player's ID. If the game's phase in "Daily Initialize", we filter the data frame `df` to consist of all actions before the `day`-th Day. Otherwise, it is set to solely include all "Talk" actions up to the `day`-th Day (inclusive). Afterwards, the predictor given by `predictor` is updated via its `update` method.

### `Action Methods`
We have implemented several methods, which perform the agent selection process required as a part of the respective action. For instance, the `vote_possessed_15` method implements the voting action of a Possessed player in a setup of 15 players. Clearly, those methods are of critical importance during the training process of our agents.

### `Training Methods`
We have implemented two training methods, `update_double_q_network` and `update_q_network`, which are utilized for training the BDQN and the DQN (respectively). Both methods follow a similar process. First, they determine the number of agents, according to which they proceed the training of the respective DQN. 

### `gradients_application(agent, tape, trainable_variables, l, main_q)`
This method receives an instance of the `PretrainedSajinAgent` class, a gradient tape (which is explained later), the trainable variables of the respective DQN, the loss incurred along the training step and `main_q` corresponds to the Q-value supplied by the online network with respect to the performed action. A gradient tape is instantiated by the `tf.GradientTape()` method, where operations are recorded if they are executed within its context manager, and at least one of their inputs is being "watched". Using `tape`, we calculate the gradients corresponding to the trainable variables, which are automatically watched. Those gradients are then incorporated for the parameters' update.  

### `get_status(diff_data, idx)`
This method returns whether the agent associated with the ID `idx` is either alive or dead, as illustrated by the data frame `diff_data`.

The `PretrainedSajinAgent` class's central methods are extensively depicted as follows:
### `initialize(self, diff\_data, n\_agents)`
Using the `game_initializer` method, the agent's role and data frame are initialized. Further, the appropriate predictor is initialized, along with a BLR layer. 

### `update(self, request, day, roleMap, is\_werewolf\_win, n\_agents)`
As a first step, we determine the current and next state of the game. Afterwards, if the agent is dead, it is penalized according to the death penalty. If `request` corresponds to the "Daily Initialize" phase, we iterate over the agent's actions. For the scenarios provided in Subsection \ref{sec:rewards}, we store the resulting experiences in a replay buffer, along with their relevant components. If `request` corresponds to the "Finish" phase, we penalize and reward the winning and losing teams (respectively), and store a terminal experience in the replay buffer. If the replay buffer contains at least a batch's size, we train the randomly selected network (either the BDQN or the DQN). Afterwards, Thompson sampling and the updating of the target network's parameters are performed if a sufficient amount of days have passed. In our own experiments, which subsequently follow, both are executed every 10 days. 

Finally, we note that the online network's parameters are saved every 20 games. The final model is saved as well.

## [`win_counter.py`]() 
Implements the winning counter metric. Its main purpose is maintaining a dictionary mapping each possible role to the number of games, in which player(s) of this role won the game. As such, this dictionary is updated at the end of each game, after the winning team is determined.

## [`Sajin.py`]() 
The actual utilization of the mentioned metrics by each relevant role is as follows:
### Seer 
A Seer incorporates both the divination metric and the coming out metric during the "Vote" phase. Specifically, if any werewolf-aligned player has been divined, he votes for one such agent. Otherwise, if there is another agent which came our as a Seer, thus it is suspected to be a werewolf-aligned player. Hence, if it wasn't previously divined as a villager-aligned player, it is voted by the Seer agent. If all agents that came out as a Seer were divined as villager-aligned players during the previous days, the Seer agent shall vote any agent which was not acknowledged as a villager.
### Villager
There can solely be a single Seer. Hence, if at least two agents came out as Seers, then at least one of them is necessarily not a Seer. Accordingly, the Villager agent shall vote such an agent which is most likely to be a werewolf. For a 15 players setup, the same applies for Medium and Bodyguard coming outs, in case there aren't enough Seer coming outs. If there aren't any such coming outs, the agents with highest probability of being a werewolf is voted.
### Werewolf
A Werewolf incorporates both the winning counter metric and the coming out metric during the "Attack" phase for a setup of 5 players. If another agent came out as a Seer during the first day, it is voted our. Otherwise, if there are any alive strong agents, one such agent is attacked. If this is not the case, we attack the agent that is most likely to be a villager.

We incorporate a mixture policy along our **Sajin** agent. First, we considered the agent put forth by the **cash** team during the [4-th Japanese AIWolf Competition](http://aiwolf.org/archives/1970). So as to improve their agent's performance, we inserted the strengthening metrics proposed earlier. Additionally, the mixture policy is further composed of the agent put forth by the **calups** team, which participated in the [1-st International AIWolf Contest](http://aiwolf.org/en/archives/2268).


# Execution

1. For executing the training phase: `python PretrainedSajin.py`
  
2. For executing the execution phase: 
* Download the latest [AIWolf server](http://aiwolf.org/en/server) 
* Windows: launch `StartServer.bat`
* Linux: execute `StartServer.sh`
* Select either 15 players or 5 players.
* Press the `connect` button.
* Run `python Sajin.py -h localhost -p 10000`
